---
title: "BIOPOLE - Chukchi Sea metatranscriptomics analysis report"
author: "Nathan"
output-file: index.html
format:
  html:
    toc: true
    toc-depth: 3
    theme: cosmo
    code-fold: true
    code-block-bg: true
editor: source
execute:
  echo: false
  warning: false
  message: false
---
<style>
pre code {
  background-color: #f2f2f2 !important;
  display: block;
  padding: 0.8em;
  border-radius: 6px;
}
code {
  background-color: #f2f2f2 !important;
  padding: 0.15em 0.35em;
  border-radius: 4px;
}
</style>

This report summarises the bioinformatics workflow applied to the Chukchi Sea eRNA samples to study the Nitrogen cycles, particularly the denitrifcation and anammox metabolic pathways, using metatranscriptomics approach.

## ðŸ§ª Sample Processing, Sequencing & Data Availability

The Centre for Genomic Research (CGR) processed **9 marine eRNA samples** (+ 1 negative control) for metatranscriptomic sequencing as part of project **SSP203072**. The workflow included:

- DNase treatment of all extracted RNA samples.

- rRNA depletion using the *Seawater riboPOOL* kit.

- Construction of **Illumina RNA-seq libraries** from the rRNA-depleted material.

Multiplexing of all libraries and sequencing on one lane of an **Illumina NovaSeq X Plus** (NovaSeq X+) 25B flow cell.

This setup is expected to produce approximately **~325 million read pairs per sample**, providing deep metatranscriptomic coverage suitable for resolving nitrogen-cycling pathways, including denitrification and anammox.

**Funding & administrative details:**

- **SSP ID**: SSP203072

- **Platform**: Illumina NovaSeq X+

- **Funding**: NEOF1819 â€” Purchase Order P11212-200

All filesâ€”including raw data, processing details, and download instructionsâ€”are available via:

[Project data download](https://cgr.liv.ac.uk/illum/SSP203072_b7beb589a74e3e61/)

## ðŸ“Š Sequencing Output & CGR QC Summary

| Name                                                                 | Raw Reads (M) | Trimmed Reads (M) | Raw Bases (M) | Trimmed Bases (M) |
|----------------------------------------------------------------------|---------------|--------------------|----------------|--------------------|
| 1-S4S                                                                | 422.03        | 418.30             | 126,608.75     | 120,507.35         |
| 2-S4M                                                                | 559.84        | 557.67             | 167,952.06     | 163,451.31         |
| 3-S4B                                                                | 613.03        | 611.42             | 183,908.71     | 179,372.50         |
| <span style="color:red;">4-S13S</span>                               | 9.89          | 7.81               | 2,968.42       | 2,244.11           |
| 5-S13M                                                               | 240.41        | 238.67             | 72,124.26      | 70,520.45          |
| 6-S13B                                                               | 617.78        | 615.94             | 185,334.01     | 181,063.09         |
| 7-S14S                                                               | 526.76        | 524.35             | 158,026.71     | 153,976.49         |
| 8-S14M                                                               | 526.33        | 524.19             | 157,899.00     | 153,074.61         |
| 9-S14B                                                               | 571.17        | 569.59             | 171,349.89     | 166,325.31         |
| <span style="color:red;">10-negative_control</span>                  | 5.92          | 5.16               | 1,776.58       | 1,487.48           |

**Description of Methods**
The raw FASTQ files are trimmed for the presence of Illumina adapter sequences using Cutadapt version 4.5 . The option -O 3 was used, meaning that trimming occurred in reads where at least 3bp of adapter sequence was observed at the 3' end of the read. The reads were further trimmed to remove low quality bases with a minimum window quality score of 20. After trimming, reads shorter than 15 bp were removed. Only read pairs are retained, therefore if a read of a pair is trimmed away then the other read in the pair is also discarded.

::: {.callout-note title="ðŸ“ Notes on Low-Yield Samples"}
**4-S13S** returned an exceptionally low number of reads because very little RNA was recovered from the corresponding Sterivex filters. This limited input material resulted in poor library complexity and low overall sequencing yield.

**10-negative_control** is a processing negative control sample and therefore contains no biological RNA. The very low read count is expected and confirms that minimal contamination occurred during library preparation.
:::

# ðŸ§ª Bioinformatics Workflow

## rRNA Removal â€“ **SortMeRNA**

To enrich for mRNA and other non-rRNA transcripts, we removed ribosomal RNA using **SortMeRNA**.

The command below shows the SortMeRNA run that was used for each sample. We used the CGR-trimmed paired-end reads as input and the **smr_v4.3_sensitive_db_rfam_seeds** database for rRNA identification.

```{bash}

sortmerna \
  --ref "smr_v4.3_sensitive_db_rfam_seeds.fasta" \
  --reads "$R1" \
  --reads "$R2" \
  --threads "$SLURM_CPUS_PER_TASK" \
  --fastx \
  --out2 \
  --paired_in \
  --aligned "${SAMPLE}_rRNA" \
  --other "${SAMPLE}_RNA_clean" \
  --workdir "$WORKDIR"
``` 

For each sample, SortMeRNA produces:

*_rRNA_* files containing reads classified as rRNA

*_RNA_clean_* files containing the rRNA-depleted read pairs, which are carried forward to assembly.

## De Novo Assembly â€“ **MEGAHIT**

We performed a co-assembly of selected rRNA-depleted samples using MEGAHIT, optimised for complex metatranscriptomic data with the meta-sensitive preset. For illustration, the command below shows the co-assembly of samples:

```{bash}
megahit 
-1 [station_samples]_RNA_clean_fwd.fq.gz 
-2 [station_samples]_RNA_clean_rev.fq.gz 
-t 60 
--presets meta-sensitive 
--min-contig-len 500 
-m 0.9 
-o St[1:3]_megahit_out
```

Assembly statistics for an example MEGAHIT run (St1) were obtained using seqkit:

```{bash}
seqkit stats final.contigs.fa
```

For example (St1 assembly):

| Assembly           | # Contigs | Total Length (bp) | Min Length (bp) | Mean Length (bp) | Max Length (bp) |
| ------------------ | --------- | ----------------- | --------------- | ---------------- | --------------- |
| `final.contigs.fa` | 1,432,902 | 1,457,453,119     | 500             | 1,017.1          | 106,426         |

These statistics indicate a highly fragmented but deep assembly, typical of complex marine microbial metatranscriptomes, with contigs â‰¥500 bp retained for downstream analyses.

## Read Mapping â€“ **Bowtie2** and **Samtools**

To quantify transcript coverage across assembled contigs, we mapped the rRNA-depleted reads back to the MEGAHIT assembly using Bowtie2, followed by BAM processing with Samtools.

First, we built a Bowtie2 index from the fixed assembly:

```{bash}
bowtie2-build St1_assembly-fixed.fa St1_assembly-fixed
```

Then, for each sample, paired-end reads were aligned and processed as follows:

```{bash}
IDX="St1_assembly-fixed"
R1="SAMPLE_RNA_clean_fwd.fq.gz"
R2="SAMPLE_RNA_clean_rev.fq.gz"
THREADS=16
OUT="St1_SAMPLE"

Map paired-end reads

bowtie2 -x "${IDX}"
-1 "${R1}"
-2 "${R2}"
-p "${THREADS}"
-S "${OUT}.sam"

Convert to BAM, keep mapped reads only (-F 4), sort and index

samtools view -F 4 -bS "${OUT}.sam" -o "${OUT}-RAW.bam"
samtools sort -@ "${THREADS}" "${OUT}-RAW.bam" -o "${OUT}.bam"
samtools index "${OUT}.bam"
```

The resulting sorted and indexed BAM files (*.bam, *.bam.bai) are used as input for downstream coverage estimation and gene- or contig-level quantification in anviâ€™o.

## Creating anviâ€™o Databases

Subsequent steps involved importing the assemblies and mapped reads into anviâ€™o to facilitate contig profiling, annotation, and interactive exploration of nitrogen-cycle genes (e.g. denitrification and anammox pathways).

In brief, the workflow included:

1. Creating a contigs database from the MEGAHIT assembly

2. Running HMMs and functional annotation (e.g. PFAM, TIGRFAMs, KOfam)

3. Profiling BAM files for each sample to obtain coverage and detection statistics

4. Merging profiles across samples for joint inspection and downstream analyses

Example (commands will vary slightly depending on environment and anvio version):


1. Create contigs database
```{bash}
anvi-gen-contigs-database -f St1_assembly-fixed.fa -o St1_contigs.db -n "St1 Chukchi Sea metaT"
```
2. Run HMMs (standard anvio collection)
```{bash}
anvi-run-hmms -c St1_contigs.db --num-threads 20
```
3. Profile each BAM file
```{bash}
anvi-profile -i St1_SAMPLE.bam -c St1_contigs.db
--output-dir St1_SAMPLE_profile
--num-threads 20
```
4. Merge profiles across samples
```{bash}
anvi-merge St1_*_profile/PROFILE.db -o St1_merged_profiles -c St1_contigs.db
```

These anviâ€™o databases form the basis for functional annotation and visualisation of key nitrogen-cycle pathways in the Chukchi Sea metatranscriptomes.

## ðŸ” Functional Annotation of genes

To characterise the metabolic potential represented in the metatranscriptomic assembly, we performed extensive functional annotation of the anviâ€™o contigs database. This step links assembled sequences to known protein families, orthologous groups, and taxonomic markers, enabling downstream interpretation of nitrogen-cycle genes (e.g., denitrification, anammox).

We used the following anviâ€™o annotation programs:

```{bash}
anvi-run-hmms
```
Identifies single-copy core genes (SCGs) and key conserved protein families using anviâ€™oâ€™s curated HMM collections. This provides quality metrics for assemblies and forms the basis for taxonomic inference.

```{bash}
anvi-run-ncbi-cogs
```
Assigns Clusters of Orthologous Groups (COGs) from NCBI to contigs based on predicted open reading frames. COG categories help identify broad functional roles such as energy production, amino acid metabolism, stress response, etc.
```{bash}
anvi-run-kegg-kofams
```
Annotates genes using KEGG Orthologs (KOfam HMMs), providing pathway-level information including nitrogen cycling modules (e.g., nitrate reduction, nitrification, denitrification, anammox).
```{bash}
anvi-run-scg-taxonomy
```
Uses single-copy core gene content to infer taxonomic origin of contigs, giving insight into which microbial groups are contributing to specific nitrogen-cycling pathways.

## ðŸ”¬ Metabolic Reconstruction and Pathway Visualisation

Following functional annotation, we performed metabolic inference and pathway-level analysis using anviâ€™oâ€™s metabolism suite. These tools integrate KEGG Ortholog (KOfam) annotations with curated pathway models to identify which biochemical pathways are represented in the dataset and to visualise how transcripts map onto specific metabolic processesâ€”particularly those involved in nitrogen cycling.

We used the following anviâ€™o programs:
```{bash}
anvi-estimate-metabolism
```
Identifies complete and partial KEGG modules across all annotated contigs, providing an overview of metabolic potential (e.g., denitrification, dissimilatory nitrate reduction, anammox).
```{bash}
anvi-react-network
```
Generates reaction-centric metabolic networks based on KEGG reactions detected in the assembly. This allows exploration of how individual reactions connect into broader nitrogen pathways.
```{bash}
anvi-reaction-network
```
Creates interactive network files linking genes (KO identifiers), reactions, and pathwaysâ€”useful for mapping expression levels or identifying pathway bottlenecks.
```{bash}
anvi-draw-kegg
```
Renders KEGG metabolic pathway diagrams annotated with detected genes, enabling visual inspection of pathway completeness and highlighting which steps are supported by metatranscriptomic evidence.

These tools collectively provide a pathway-level view of nitrogen cycling activity in the Chukchi Sea microbial community, enabling detailed interpretation of expression patterns associated with denitrification, nitrification, and anammox processes.

## KEGG Module M00529 â€” Denitrification (nitrate â†’ nitrogen)

![KEGG module M00529 (Denitrification, nitrate â†’ nitrogen). Diagram from https://www.genome.jp/dbget-bin/www_bget?md:M00529](figures/KEGG_Module_Denitrification_M00529.png){width=75%}

From the output of anvi-estimate-metabolism, we can look for the KEGG module for Denitrification (M00529) and look at its completeness:

| KEGG Module | Station   | stepwise_module_completeness | pathwise_module_completeness |
|------------|-----------|--------------|------------|
| M00529     | Station_1 | 1.00         | 1.00       |
| M00529     | Station_2 | 1.00         | 1.00       |
| M00529     | Station_3 | 1.00         | 1.00       |

So all the genes required for denitrification to occur seem to be detected at each of the stations.

Let's look at the denitrification pathway in more details using anvi-interactive, so we can see the coverage of the genes involved in denitrification across the 3 depths in the stations.

First, we need to create an anvi'o collection that contains all the denitrification genes. Using **anvi-export-functions** we can get the *gene_callers_ids* matching your *KO IDs*, then we can get the contigs name using **anvi-export-table --table genes_in_contigs**. After that we need to use *join* to find the contigs that have the genes of interest and finally we need to add "_split_00001" to each contig name (because in this case each contig has only one split as these come from metaT assembly. We ad a column with the name of our bin and then can import that using **anvi-import-collection** 

```{bash}
anvi-export-functions   -c St1_contigs.db   -o St1_gene_functions.tsv
anvi-export-table --table genes_in_contigs -o St1_genes_in_contigs.tsv St1_contigs.db
tail -n +2 St1_genes_in_contigs.tsv | cut -f1,2 > gene_to_contig.tsv

# KO list for denitrification (edit/add as needed)
KOS='K00370|K00371|K00374|K02567|K02568|K00368|K15864|K04561|K02305|K00376'
# Get unique gene_callers_id hits for those KOs
awk -F'\t' -v kos="$KOS" '($3 ~ kos){print $1}' St1_gene_functions.tsv   | sort -u > genes_of_interest.ids
join -t $'\t' genes_of_interest.ids gene_to_contig.tsv   | cut -f2   | sort -u > contigs_of_interest.txt
awk '{print $1"_split_00001\tdenitrification"}' contigs_of_interest.txt   > denitrification_splits_collection.tsv

anvi-import-collection   -p St1_MERGED/PROFILE.db   -c St1_contigs.db   -C denitrification_splits denitrification_splits_collection.tsv
```
Now that we have an anvio collection, we need to "split" it from the main contigs and profile databases (we create a subset). This is because our initial contigs.be has too many splits (1,432,906 splits to be precise) and hierarchical clustering gets turned off.

Once it is splitted we can find a directory called "denitrifications_splits" in which we find a contigs and profile database that we can now plot using anvi-interactive:

## anvi'o interactive plot - Denitrification - Nitrate reductase - Station 1

![Anvi'o interactive plot](figures/anvi_plot_dinitrification_nitrate_reductase.png){width=100%}

## anvi'o interactive plot - Denitrification - Nitrite reductase (nirK) - Station 1

![Anvi'o interactive plot](figures/anvi_plot_denitrification_nirK.png){width=100%}

Now we can start looking for specific genes and creating bins for each so we can visualise them and see their coverage.

## KEGG Map of Nitrogen Pathways

![KEGG map of Nitrogen metabolic pathways. Diagram from anvi'o and KEGG. In green are the genes present in anvio contig database for Station 1](figures/Nitrogen_metabolism_St1_KEGG-1.png){width=100%}

##  KEGG nitrogen cycle (map00910)

This is a standard, widely used KO set covering:

- Nitrification
- Denitrification
- DNRA
- Anammox
- Nitrogen fixation

Assimilatory pathways

###  Core nitrogen-cycle KOs (map00910)
- Nitrogen fixation
K02588  nifH
K02591  nifD
K02586  nifK

- Nitrification
K10944  amoA
K10945  amoB
K10946  amoC
K10535  hao

- Assimilatory nitrate reduction
K00367  nasA
K00360  nirA

- Dissimilatory nitrate reduction (DNRA)
K00362  napA
K00363  napB
K00370  narG
K00371  narH
K00374  narI
K02567  nrfA
K02568  nrfH

- Denitrification
K00368  nirK
K15864  nirS
K04561  norB
K02305  norC
K00376  nosZ

- Anammox
K20932  hzsA
K20933  hzsB
K20934  hzsC
K20935  hdh







## Tara Oceans Arctic data as reference

### Rationale and overview

As no site-matched metagenomic data were available for these samples, a reference-based approach was adopted using publicly available metagenomic data from the Tara Oceans Arctic expedition (**Station 194**), representing the closest comparable marine Arctic microbial communities currently available.

![Map of Station 194 of the Tara Oceans Polar Circle expedition in 2013](figures/Tara_Ocean_Artic_St_194_map.png)
2:45 AM on September 8, 2013 (UTC)

### Station 194 - Tara Oceans Polar Circle expedition

| Field | Value |
|-------|-------|
| BioSamples ID | SAMEA4397903 |
| ENA Run ID | ERR3589571 |
| Date/Time | 2013-09-12T03:59:00Z |
| Latitude | 73.3275 |
| Longitude | -168.8142 |
| Depth | 35 |
| Layer | DCM |
| Size fraction | 0.22â€“3 |
| Region | Arctic Ocean |



This strategy enables the detection and quantification of expressed genes that are conserved between Arctic marine microbial communities, while acknowledging that locally specific taxa or gene variants not present in the reference dataset may not be captured.

### Assembly of Tara Oceans Arctic metagenomes

Raw metagenomic reads from the Tara Oceans Arctic expedition were assembled de novo using **MEGAHIT**, which is optimized for large and complex metagenomic datasets. The resulting assembly produced a set of contigs representing microbial genomic fragments present in Arctic marine surface waters.

These contigs were subsequently used as a reference database for read mapping. While the Tara Oceans sampling locations do not overlap geographically with the Chukchi Sea, they provide a comprehensive representation of Arctic marine microbial diversity and metabolic potential, particularly for conserved biogeochemical pathways such as nitrogen cycling.

### Metatranscriptomic read processing and mapping

Quality-filtered metatranscriptomic reads were mapped to the Tara Oceans Arctic contigs using **Bowtie2** with default end-to-end alignment settings. Mapping was performed in paired-end mode to improve alignment specificity. Both concordant and discordant alignments were reported, although downstream analyses focused on concordantly mapped read pairs.

**Mapping performance and alignment statistics**

Across samples, overall **alignment rates ranged from 15.8% to 27.3%** of paired-end reads mapping to the Tara Oceans Arctic contigs. The majority of reads aligned concordantly exactly once (11â€“18%), with a smaller proportion aligning to multiple locations (3â€“6%). Discordant alignments accounted for approximately 0.5â€“1% of read pairs.

These mapping rates are consistent with expectations for metatranscriptomic datasets mapped to non-site-matched metagenomic references, particularly in polar marine systems where microbial communities exhibit strong spatial structure and strain-level diversity. The observed alignment patterns suggest that a substantial fraction of expressed genes in the Chukchi Sea microbial community are shared with, or closely related to, those represented in the Tara Oceans Arctic metagenomes.

**Interpretation and limitations**

This reference-based approach enables the identification and relative quantification of expressed genes that are conserved across Arctic marine microbial communities, including key genes involved in nitrogen cycling. However, transcripts derived from locally unique taxa or divergent gene variants absent from the Tara Oceans reference assembly are unlikely to be captured, resulting in an underestimation of total transcript diversity and expression.

Consequently, analyses based on this approach focus on relative expression patterns of nitrogen-cycle gene families represented in the reference dataset, rather than absolute transcript abundances or the absence of specific metabolic pathways. Despite these limitations, the approach provides a robust framework for exploring nitrogen cycling activity in the Chukchi Sea in the absence of site-specific metagenomic data.
